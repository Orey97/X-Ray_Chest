{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest X-Ray Training Pipeline (Robust V3)\n",
    "\n",
    "**Updates:** Improved column matching logic (V3) to catch 'image' vs 'image_index'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. MOUNT DRIVE\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    print(\"üîå Connecting to Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "    print(\"üíª Running Locally\")\n",
    "\n",
    "# 2. AUTO-DISCOVERY \n",
    "def find_file(filename, search_path):\n",
    "    print(f\"üîç Searching for '{filename}' in {search_path}...\")\n",
    "    matches = sorted(list(Path(search_path).rglob(filename)))\n",
    "    if matches:\n",
    "        print(f\"   ‚úÖ Found: {matches[0]}\")\n",
    "        return matches[0]\n",
    "    return None\n",
    "\n",
    "# 3. SETUP PATHS\n",
    "if IS_COLAB:\n",
    "    SEARCH_ROOT = \"/content/drive/My Drive\"\n",
    "    WORK_DIR = Path(\"/content/work\")\n",
    "else:\n",
    "    SEARCH_ROOT = \"..\"\n",
    "    WORK_DIR = Path(\"./temp_work\")\n",
    "\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "ZIP_PATH = find_file(\"images-224.zip\", SEARCH_ROOT)\n",
    "if not ZIP_PATH: raise FileNotFoundError(\"Please upload images-224.zip to Drive\")\n",
    "\n",
    "IMAGE_DIR = WORK_DIR / \"images-224\"\n",
    "if not IMAGE_DIR.exists():\n",
    "    print(\"‚è≥ Unzipping...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
    "        z.extractall(WORK_DIR)\n",
    "    print(\"‚úÖ Unzip Done.\")\n",
    "\n",
    "CSV_PATH = find_file(\"Data_Entry_2017.csv\", SEARCH_ROOT)\n",
    "if not CSV_PATH: CSV_PATH = find_file(\"Data_Entry_2017.csv\", WORK_DIR)\n",
    "if not CSV_PATH: raise FileNotFoundError(\"Please upload Data_Entry_2017.csv to Drive\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ROBUST DATA PROCESSING\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(\"üìä Loading CSV...\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# --- DEFENSIVE COLUMN CLEANING ---\n",
    "# 1. Normalize all columns to lower_snake_case\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "print(f\"   Columns detected: {list(df.columns)}\")\n",
    "\n",
    "# 2. Identify Key Columns dynamically (Improved Logic)\n",
    "def match_col(cols, keywords):\n",
    "    for c in cols:\n",
    "        if all(k in c for k in keywords): return c\n",
    "    return None\n",
    "\n",
    "# Priority 1: 'image' AND 'index'. Priority 2: just 'image'\n",
    "img_col = match_col(df.columns, ['image', 'index']) or match_col(df.columns, ['image'])\n",
    "# Priority 1: 'patient' AND 'id'. Priority 2: just 'patient'\n",
    "id_col  = match_col(df.columns, ['patient', 'id']) or match_col(df.columns, ['patient']) \n",
    "lbl_col = match_col(df.columns, ['label']) or match_col(df.columns, ['finding'])\n",
    "\n",
    "if not img_col or not id_col or not lbl_col:\n",
    "    raise ValueError(f\"Could not identify columns automatically. Found: {list(df.columns)}\")\n",
    "\n",
    "print(f\"   Mappings: Image='{img_col}', ID='{id_col}', Label='{lbl_col}'\")\n",
    "\n",
    "# 3. Process Labels\n",
    "df[lbl_col] = df[lbl_col].astype(str).str.split('|')\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded = mlb.fit_transform(df[lbl_col])\n",
    "classes = mlb.classes_\n",
    "print(f\"   Classes Found ({len(classes)}): {classes}\")\n",
    "\n",
    "# 4. Construct Final DataFrame\n",
    "df_enc = pd.DataFrame(encoded, columns=classes)\n",
    "# Robust concat: Use the dynamically found column names\n",
    "df = pd.concat([df[[img_col, id_col]], df_enc], axis=1)\n",
    "# Now standardize names for the Dataset class\n",
    "df.rename(columns={img_col: 'image', id_col: 'patientid'}, inplace=True)\n",
    "\n",
    "# 5. Dataset Class\n",
    "class ChestXRayDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, tf=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.tf = tf\n",
    "        self.img_names = df['image'].values\n",
    "        # Drop metadata to keep only One-Hot labels\n",
    "        self.labels = df.drop(['image', 'patientid'], axis=1).values.astype('float32')\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.img_names[idx]\n",
    "        path = os.path.join(self.img_dir, name)\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            if self.tf: img = self.tf(img)\n",
    "            return img, torch.tensor(self.labels[idx])\n",
    "        except:\n",
    "            # Fail gracefully, but maybe print warning only one time\n",
    "            return torch.zeros((3,224,224)), torch.tensor(self.labels[idx])\n",
    "\n",
    "# 6. Split & Loaders\n",
    "pats = df['patientid'].unique()\n",
    "train_p, test_p = train_test_split(pats, test_size=0.15, random_state=42)\n",
    "train_p, val_p  = train_test_split(train_p, test_size=0.15, random_state=42)\n",
    "\n",
    "train_df = df[df['patientid'].isin(train_p)]\n",
    "val_df   = df[df['patientid'].isin(val_p)]\n",
    "\n",
    "print(f\"   Train: {len(train_df)}, Val: {len(val_df)}\")\n",
    "\n",
    "train_tf = transforms.Compose([transforms.Resize((224,224)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "val_tf   = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_loader = DataLoader(ChestXRayDataset(train_df, IMAGE_DIR, train_tf), batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(ChestXRayDataset(val_df, IMAGE_DIR, val_tf), batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"‚úÖ Pipeline Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. TRAINING LOOP\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.base = models.resnet50(pretrained=True)\n",
    "        self.base.fc = nn.Linear(self.base.fc.in_features, n_classes)\n",
    "    def forward(self, x): return self.base(x)\n",
    "\n",
    "model = ResNet50(len(classes)).to(device)\n",
    "crit = nn.BCEWithLogitsLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "EPOCHS = 10\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(\"üî• Starting Training...\")\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {ep+1}/{EPOCHS}\")\n",
    "    \n",
    "    for imgs, lbls in loop:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = crit(out, lbls)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in val_loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            val_loss += crit(model(imgs), lbls).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"   Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save Best\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        if IS_COLAB:\n",
    "            save_path = f\"/content/drive/My Drive/xray_best_model.pth\"\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"   üíæ Best Model Saved to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
